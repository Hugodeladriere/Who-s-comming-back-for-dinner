{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kaggle Who's comming back for dinner.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Le but de ce projet est de prédire si un client ayant fait une réservation dans un restaurant est susceptible de revenir dans le restaurant.\n<br>\nPour cela, nous avons une base de données qui nous renseigne sur les réservations, sur les restaurants et sur les clients.\n<br>\nLe projet peut être écrit en R ou en python. Le choix de langage est le python","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Sommaire\n<a href= \"#Importer_les_données\"> 1. Importer les données </a>\n<br>\n<a href= \"#Evaluer_les_données\"> 2.Évaluer la qualité des données et les valeurs manquantes </a>\n<br>\n<a href= \"#Ajustement\"> 3.Ajustement des données </a>\n<p style=\"text-indent:20px;\">  <a href= \"#Ajustement_datetime\"> 3.1.Ajustement de la colonne datetime </a>  </p>\n<p style=\"text-indent:20px;\">  <a href= \"#Ajustement_status\"> 3.2.Ajustement de la colonne status </a>  </p>\n<p style=\"text-indent:20px;\">  <a href= \"#Ajustement_purpose\"> 3.3.Ajustement de la colonne purpose </a>  </p>\n<p style=\"text-indent:20px;\">  <a href= \"#Ajustement_autres\"> 3.3.Ajustement d'autres colonnes</a>  </p>\n<a href= \"#Ajout_restaurant\"> 4. Ajout de nouvelles données </a>\n<br>\n<a href= \"#preparation\">  5.Préparation des données </a>\n<br>\n<a href= \"#Execution\">  6.Exécution de simples algorithmes de régression linéaire </a>\n<br>\n<a href= \"#AUC\">  7. Théorie de la courbe ROC </a>\n<br>\n<a href= \"#randomclassifier\">  8. Algorithme RandomClassifier </a>\n<br>\n<a href= \"#Logistic\">  9. Algorithme régression logistique </a>\n<br>\n<a href= \"#boost\">  10. Algorithme régression logistique et boosting  </a>\n<br>\n\n\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Voici la liste des packages susceptibles d'être utilisés dans un projet de Deep Learning. Nous pouvons mettre en lumière deux bibliothèques les plus essentielles: \n    - Numpy qui crée une Data Frame\n    - Scikit-learn qui sert notamment à l'apprentissage automatique des données en fonction d'algorithmes mathématiques ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Main packages\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\n\n# Graph package \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Classifiers\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingRegressor\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n#from sklearn.neighbors import KNeighborsClassifier\n#from sklearn.svm import SVC\n#from sklearn.tree import DecisionTreeClassifier\n\nfrom xgboost import XGBClassifier\n#from sklearn.feature_selection import SelectKBest ,chi2,RFE\n\n\n# Tools\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\n\n#Yes pourquoi pas \nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import make_scorer, accuracy_score, roc_auc_score\n\n\n#Noramlize your Data bro\nfrom sklearn.preprocessing import StandardScaler\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p id=\"Importer_les_données\"> 1.Importer les données </p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Dans un premier temps, nous exportons tous les datas .csv en data frame avec la bibliothèque numpy\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing data\ntest = pd.read_csv('../input/data-kaggle/Test.csv')\ntrain = pd.read_csv('../input/data-kaggle/Train.csv')\n\n#Add extra data on your Data Set \nrestaurant = pd.read_csv('../input/data-kaggle/restaurant.csv', sep=';', encoding='utf8')\n#members = pd.read_csv('Data/member.csv', sep=';', encoding='utf8')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous travaillons dans un premier temps avec deux data frame, Train et Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#preview test data\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Le nombre de ligne dans le tableau test est :{}.'.format(test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note :  il n'y a pas de variable cible dans les données de test (c'est-à-dire que la colonne \"Return90\" est manquante), l'objectif est donc de prévoir cette cible en utilisant différents algorithmes d'apprentissage.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n# <p id=\"Evaluer_les_données\"> 2.Évaluer la qualité des données et les valeurs manquantes </p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Quand on se confronte pour la première fois à un projet de machine learning, on peut être tenté de croire que la problématique principale s’articule autour de l’algorithme. S’il est vrai que cette partie doit être parfaitement maîtrisée pour permettre aux prédictions d’atteindre des résultats d’une fiabilité suffisamment élevée pour être exploitables, le choix et le paramétrage des algorithmes ne représentent que peu souvent la problématique la plus complexe. Le résultat optimal peut être atteint seulement avec un bon nettoyage des données.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Les valeurs nulles trompent les algorithmes de prédictions et faussent le résultat. Il faut donc tout d'abord les traiter et les remplacer.\n<br>\nNous remarquons que les valeurs nulles sont trouvées dans la colonne \"purpose\"\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train\\n',train.isnull().sum())\nprint('.................')\nprint('.................')\nprint('Test\\n',test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['purpose'].value_counts().plot(kind='barh', figsize=(6,6))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Le pourcentage de valeurs nulles dans la colonne \"purpose\"  est  %.2f%% ' %((train['purpose'].isnull().sum()/train.shape[0])*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous définissons alors que si un client n'a pas rempli la colonne 'purpose\", alors il a fait une réservation sans aucune intention particulière.\n<br>\nDonc toutes les valleurs nulles sont remplacées par le but 'Aucun'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_na(df): \n    df['purpose'] = df['purpose'].fillna('Aucun')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p id=\"Ajustement\"> 3. Ajustement des données </p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"La data base fournie des données brutes, il faut donc les traiter pour leur donner du sens ou bien ajouter des paramètres à l'algorithme.\n<br>\nNous traiterons alors les colonnes : 'purpose', 'datetime', 'gender', 'status' et nous supprimons la colonne 'cdate' qui n'apporte aucune information  à l'algorithme  de prédiction\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n# <p id=\"Ajustement_datetime\"> 3.1. Ajustement de la colonne datetime </p>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"datetime\"].head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La colonne datetime nous présente des données en timestamp, inconvénient pour un algorithme de prédiction. Cependant,  nous pouvons en extraire le mois et l'heure. \n<br>\nLa bibliothèque 'pandas permet d'extraire ces informations avec la fonction 'DatetimeIndex'.\n<br>\nNous calculons l'heure de la réseravtion(diner, déjeuner), le mois et aussi le nombre de jours d'avance que le client à réservé. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean la colone datetime en 4 colonnes, month, heure du déjeuner, heure du souper, nombre de jours réservation avant \ndef date(df):\n    df['d1'] = pd.to_datetime(df['datetime'])\n    df['month']=pd.DatetimeIndex(df['d1']).month\n    df['dejeuner']= pd.DatetimeIndex(df['d1']).hour< 17\n   \n    df['diner']= (pd.DatetimeIndex(df['d1']).hour >=17)\n\n    df['nb_jours_avance'] = ((df[\"d1\"]) - pd.to_datetime(df['cdate_x'])).dt.days\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p id=\"Ajustement_status\"> 3.2. Ajustement de la colonne status </p>\n","execution_count":null},{"metadata":{},"cell_type":"raw","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La colonne \"status\" nous donne des indications sur le statut de la réservation; la variable peut prendre plusieurs valeurs : 'changed', 'no-show', 'new','canceled', 'ok'.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['status'].value_counts().plot(kind='barh', figsize=(5,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous utilisons les 'dummy', cette fonction permet de convertir une colonne et ses X variables, en X nouvelles colonnes booléennes dont les titres sont les variables ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.get_dummies( train['status']).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initialement, nous voulons créer seulement deux colonnes, si le client se présente ou ne se présente pas.\n<br>\nMais avec un peu plus de recul, l'information brute apporte plus d'informations à l'algorithme.\n<br>\nFinalement, la solution finale est juste un dummy de la colonne statut.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def  status(df):\n\n    dummy = pd.get_dummies( df['status'])\n    #dummy ['present'] = dummy['changed']+dummy['ok']+ dummy['new']\n    #dummy['absent'] = dummy['no-show']+dummy['canceled']\n\n    #dummy.drop('canceled',inplace=True, axis=1)\n    #dummy.drop('changed',inplace=True, axis=1)\n    #dummy.drop('new',inplace=True, axis=1)\n    #dummy.drop('no-show',inplace=True, axis=1)\n    #dummy.drop('ok',inplace=True, axis=1)\n    \n    return dummy\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p id=\"Ajustement_purpose\"> 3.3. Ajustement de la colonne purpose</p>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test['purpose'].value_counts().head(20).plot(kind='barh')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"L'ajustement de la colonne \"purpose\" nous a donné du fil à retordre. La première solution apportée est de sélectionner seulement les variables utiles. En effet, après visualisation du graphe ci-dessus, il y a seulement 6 variables interférentes sur 16. Après les avoir sélectionnées il suffit de l'insérer dans notre tableau final (voir fonction ci-dessus). Cette fonction n'a pas été retenue, puisque lors du rendu final du projet, le site Kaggle exigeait le nombre initial de lignes. Ainsi, nous avons compris que nous ne pouvons pas supprimer de lignes ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def purpose(df):\n    df.drop((df[~df['purpose'].str.contains('anniversaire')].index) & \n            (df[~df['purpose'].str.contains('amis')].index) & \n            (df[~df['purpose'].str.contains('famille')].index) & \n            (df[~df['purpose'].str.contains('Sweet')].index) & \n            (df[~df['purpose'].str.contains('affaires')].index) & \n            (df[~df['purpose'].str.contains('importante')].index) & \n            (df[~df['purpose'].str.contains('Aucun')].index), inplace = True)\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La solution retenue est d'utiliser les \"dummy\".Ainsi, créer un tableau temporaire booléen, et de les réunir par sujet (anniversaire, amis, famille....)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def dummy_purpose_train(df):\n    dummy = pd.get_dummies(df['purpose'])\n    df['anniversaire']= (dummy['Birthday']== 1) | (dummy['Birthday,Celebration']== 1 ) | (dummy[\"Célébration d'anniversaire\"]==1) | (dummy[\"Fête d'anniversaire\"]==1)\n    df['amis']=(dummy['Friends,Reunion']== 1) | (dummy['Dîner entre amis']== 1 )\n    df['famille'] = (dummy['Family,Gathering']== 1) | (dummy['Repas en famille']== 1 )| (dummy['Repas en famille,盡量靠W']== 1 )| (dummy['Repas en famille-生日']== 1 ) \n    df['business'] = (dummy[\"Dîner d'affaires\" ]== 1)\n    df['Date_tinder'] = (dummy['Sweet day' ]== 1)\n    df['importante'] = (dummy['Date importante' ]== 1)\n    df['autre']=(dummy['Aucun']== 1) |(dummy['Veuillez sélectionner']== 1) | (dummy['其他']== 1) |(dummy['Please,Select']== 1) | (dummy['&#65533;&#40115;&#65533;&#65533;&#26813;&#65533;']== 1) | (dummy['家人&#65533;']== 1)\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dummy_purpose_test(df):\n    dummy = pd.get_dummies(df['purpose'])\n    df['anniversaire']= (dummy[\"Fête d'anniversaire\"]== 1) | (dummy[\"Célébration d'anniversaire\"]== 1)\n    df['amis'] = (dummy[\"Dîner entre amis\"]== 1) | (dummy[\"Friends,Reunion\"]== 1)| (dummy[\"Friends\"]== 1)\n    df['famille'] = (dummy[\"Repas en famille\"]== 1) | (dummy[\"Family,Gathering\"]== 1)\n    df['business'] = (dummy[\"Dîner d'affaires\" ]== 1)| (dummy[\"Business,meeting\"]== 1)\n    df['Date_tinder'] =(dummy['Sweet day' ]== 1)\n    df['importante'] = (dummy['Date importante' ]== 1)\n    df['autre']=(dummy['Aucun']== 1) |(dummy['Veuillez sélectionner']== 1) | (dummy['其他']== 1) |(dummy['Please,Select']== 1) |(dummy['慶生']== 1) | (dummy[\"&#65533;&#40115;&#65533;&#65533;&#26813;&#65533;\"]== 1)\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cependant, cette partie du code pose des problèmes dans la suite du projet et dans la maintenance. En effet, si les restaurants ajoutent d'autres variables, alors le programme ne sera pas en mesure de les prendre en compte.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <p id=\"Ajustement_autres\"> 3.4. Ajustement des autres colonnes</p>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Nous avons aussi modifier de simple colone pour les rendre bouléennes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def gender(df):\n    dummy = pd.get_dummies( df['gender'])\n    df['Femme']=(dummy['F']==1)\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def join(df1,df2):\n    df3  = df1.join(df2, how='inner')\n    return df3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def present(df):\n    df['Present']= (df['present']==1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p id=\"Ajout_restaurant\"> 4.Ajout de nouvelles données</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Afin d'avoir des résultats optimaux, nous avons rajouté des informations à nos bases de données. Le tableau \"restaurant\" qui apporte des indications sur les restaurants. \nLa fusion des deux tableaux se fait sur la clé unique 'restaurant_id' et l 'id'. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add csv Restaurant on your Train and Test data set\nresultat_train = pd.merge(train, restaurant, left_on= 'restaurant_id', right_on='id',how='left')\nresultat_test = pd.merge(test, restaurant, left_on= 'restaurant_id', right_on='id',how='left')\n\nresultat_test=resultat_test.set_index('booking_id')\nresultat_train=resultat_train.set_index('booking_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En ajoutant de nouvelles données, il est nécessaire de recommencer toutes les étapes de nettoyage. \n<br>\nEnlever toutes les valeurs nulles, identifier les valeurs utiles et faire des ajustements si nécessaire","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"resultat_train.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Après avoir identifié les colonnes qui contiennent des variables nulles, il faut les remplacer par des valeurs logiques.\n\nLes variables  'is_hotel', 'good_for_family', 'accept_credit_card', 'parking', 'outdoor_seating', 'wifi', 'wheelchair_accessible' sont remplacées par 0 si elles ne sont pas indiquées.\n\nLes variables 'price1 ' et 'price2' sont remplacées par une moyenne des deux. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_resultat(df):\n    df['is_hotel'] = df['is_hotel'].fillna(0)\n    df['good_for_family'] = df['good_for_family'].fillna(0)\n    df['accept_credit_card'] = df['accept_credit_card'].fillna(0)\n    df['parking'] = df['parking'].fillna(0)\n    df['outdoor_seating'] = df['outdoor_seating'].fillna(0)\n    df['wifi'] = df['wifi'].fillna(0)\n    df['wheelchair_accessible'] = df['wheelchair_accessible'].fillna(0)\n\n\n    df['price1'] = df['price1'].fillna(df.price1.mean())\n    df['price2'] = df['price2'].fillna(df.price2.mean())\n    df['cdate_y'] = df['cdate_y'].fillna('Aucun')\n\n\n    df['id'] = df['id'].fillna('Pasid')\n    \n    \n    df['purpose'] = df['purpose'].fillna('Aucun')\n    \n    return df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Toutes les informations ci-dessous sont utiles pour  l'algorithme de prédiction. Cependant, si nous lançons cet algorithme avec ces données le résultat ne sera pas optimal.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"restaurant.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous créons deux nouvelles colonnes :\n  - accessible qui comprend les variables 'parking' et 'wheelchair_accesible'\n  - agréable qui comprend les variables 'accept_credit_card', 'wifi' et 'outdoor_seeting'\n    \n    \nNous créons aussi un prix moyen de la carte du restaurant. Nous catégorisons si ce restaurant est luxueux, normal ou accessible (avec les paramètres 'prix_haut', 'prix_moyen', 'prix_bas' respectivement)\n\n\nNous prenons aussi en compte la date du restaurant pour déterminer s'il est vieux ou jeune. Un restaurant est défini comme vieux lorsqu'il a plus de 5 ans.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def modif_restaurant(df):\n\n    df['accesible']= df[\"parking\"] + df[\"wheelchair_accessible\"]\n    df['agréable']= df[\"accept_credit_card\"] + df[\"wifi\"]+ df[\"outdoor_seating\"]\n\n\n    df['prix'] = (df['price1']+df['price2'])/2\n    \n    \n    df['prix_haut']=df['prix']>=800\n    df['prix_moyen']=(df['prix']>=488) & (df['prix']<800)\n    df['prix_bas']=df['prix']<488\n    \n    \n    df[\"date\"] = 2020 - (pd.to_datetime(df[\"cdate_y\"],errors='coerce')).dt.year\n    \n    df['date'] = df['date'].fillna(df.date.mean())\n    \n    df['vieux_resto']=df['date']>=5\n    df['jeune_resto']=df['date']<5\n    return df\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p id=\"preparation\"> 5. Préparation des données</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Enfin nous exécutons les commandes pour la base Train et la base Test et nous supprimons les variables inutiles.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(df):\n    \n    gender(df)\n    \n    \n    df4 = status(df)\n    df5 = join(df,df4)\n    \n    df6 = date(df5)\n    df7=modif_restaurant(df6)\n    \n    \n    colums = ['member_id', 'cdate_x', 'restaurant_id', 'datetime','purpose','gender',\n              'status','is_required_prepay_satisfied', 'd1','cdate_y','id',\n              'parking','wheelchair_accessible','accept_credit_card','wifi','outdoor_seating',\n              'price1','price2','prix','date']\n    \n    df7.drop(colums,inplace=True, axis=1)\n    \n    return df7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_resultat(resultat_train)\nfill_resultat(resultat_test)\n\n\ntest = dummy_purpose_test(resultat_test)\ntrain = dummy_purpose_train(resultat_train)\n\ndf_test=prepare_data(test)\ndf_train=prepare_data(train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Voici le résultat final de la base Test:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Avant de passer aux algorithmes nous séparons notre base en 2/3 d'apprentissage et en 1/3 de test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creer une base X_train y_train et X_test et y_test (divise en 2/3 apprentissage, 1/3 test)\nX_all = df_train.drop(['return90'], axis=1)\ny_all = df_train['return90']\n\nnum_test = 0.33\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=num_test, random_state=2)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # <p id=\"Execution\"> 6.Exécution de simple algorithme de regression linéaire</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Pour commencer nous exécutons un algorithme de régression linéaire.\n\nLa régression linéaire est un algorithme de machine learning qui consiste à trouver la meilleure fonction permettant de définir une variable de sortie (l’élément à prédire) à partir d’une  seule et unique variable explicative en entrée (le prédicteur).Graphiquement, il s’agit de trouver la meilleure droite possible pouvant expliquer un modèle (x, y). Cette fonction est de la forme Y = BO + XB1 + E, où Y est la réponse, BO l’ordonnée à l’origine, X la variable explicative, B1 la pente (la valeur d’évolution de X lorsqu’il augmente d’une unité), et enfin E, l’erreur statistique.\n\nVous trouvez ci-dessous, un algorithme simple. Cependant, le Kaggle ne calcule pas le score selon une simple droite, mais selon l'aire sous la courbe ROC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# build\nlinreg = LinearRegression()\n\n# train\nlinreg.fit(X_train, y_train)\n\n# predict\ny1_pred = linreg.predict(df_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # <p id=\"AUC\"> 7. Théorie de la courbe ROC</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Le Kaggle détermine le score en fonction de l'aire sur la courbe ROC.\n\nUne courbe ROC (receiver operating characteristic) est un graphique représentant les performances d'un modèle de classification. Cette courbe trace le taux de vrais positifs en fonction du taux de faux positifs :\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://developers.google.com/machine-learning/crash-course/images/ROCCurve.svg\" height=\"50%\" width=\"50%\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# AUC : aire sous la courbe ROC\n<br>\nAUC signifie \"aire sous la courbe ROC\". Cette valeur mesure l'intégralité de l'aire sous la courbe ROC .\n\n<img src=\"https://developers.google.com/machine-learning/crash-course/images/AUC.svg\" height=\"50%\" width=\"50%\">\n\nL'AUC fournit une mesure  des performances pour tous les seuils de classifications possibles. On peut interpréter l'AUC comme une mesure de la probabilité pour que le modèle classe un exemple positif aléatoire au-dessus d'un exemple négatif aléatoire. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" # <p id=\"randomclassifier\"> 8.Algortithme Random Classifier</p>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Les forêts d'arbres décisionnels (ou random forest classifier) font partie des techniques d'apprentissage automatique. Cet algorithme combine les concepts de sous-espaces aléatoires et de bagging. L'algorithme random forest effectue un apprentissage sur de multiples arbres de décision entraînés sur des sous-ensembles de données légèrement différents.\n\nLes paramètres de l'algorithme sont disponibles sur la documentation de Scikit-learn. Cependant pour choisir les bons paramètres, il faut les sélectionner par dichotomie. Par exemple, le paramètre: \n<br>\n-->'n_estimators': [ 300,400,500]\n\nL'algorithme exécute avec les 3 possibilités, et nous retourne la valeur idéale pour nos données (ici n_estimators=300). Ainsi au fur et à mesure, nous trouvons les paramètres idéaux pour l'algorithme.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choose the type of classifier. \nclf = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [ 300,400,500], \n              'max_features': ['auto' ], \n              'criterion': ['gini', 'entropy'],\n              'max_depth': [10],\n              'min_samples_split': [500],\n              'min_samples_leaf': [2]\n             }\n\n\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(roc_auc_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer, n_jobs=-1)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\nprint(clf)\n\n# Fit the best algorithm to the data. \nbest_clf = clf.fit(X_train, y_train)\nprint('Train score', clf.score(X_train, y_train))\nprint('Test score', clf.score(X_test, y_test))\n\n\n#plus proche voisin,  svm\n#regarder la mtrice de confusion avec train test split\n#AUC detection ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = best_clf.predict_proba(X_test)[:,1]\n\n\nfpr, tpr, _ = roc_curve(y_test,y_pred)\nplt.plot(fpr, tpr, marker='.', label='LogistiIIc')\n\n\nprint(\"Le score AUC est: {} \".format(roc_auc_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # <p id=\"Logistic\"> 9.Algorithme Régression Logistique </p>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Nous fonctionnons avec la même logique pour la régression logistique ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"regl = LogisticRegression()\n\nparameters = {'penalty': ['l2'],\n              'max_iter': [1000],\n              'C': [3.0],\n              'solver' :[ 'newton-cg','sag', 'lbfgs'],\n              'multi_class':['auto', 'ovr', 'multinomial']\n             }\n\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(roc_auc_score)\n\n\n\n\n# Run the grid search\ngrid_obj = GridSearchCV(regl, parameters, scoring=acc_scorer, n_jobs=-1, cv=None)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nregl = grid_obj.best_estimator_\nprint(regl)\n\n# Fit the best algorithm to the data. \n\nregl.fit(X_train, y_train)\nprint('Train score', regl.score(X_train, y_train))\nprint('Test score', regl.score(X_test, y_test))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y1_pred = regl.predict_proba(X_test)[:,1]\n\n\nfpr, tpr, _ = roc_curve(y_test,y_pred)\nplt.plot(fpr, tpr, marker='.', label='LogistiIIc')\n\n\nprint(\"Le score AUC est: {}\".format(roc_auc_score(y_test, y1_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # <p id=\"boost\"> 10.Algorithme Régression logistique et boosting  </p>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"La méthode du gradient boosting sert à renforcer un modèle qui produit des prédictions faibles.\n<br>\nAinsi nous choisissons aussi les paramètres du gradient boosting qui nous propose des paramètres idéaux pour la régression linéaire","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = XGBClassifier()\n\n\n\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'booster':['gbtree'],\n              'learning_rate': [0.02], #so called eta value\n              'max_depth': [18],\n              'min_child_weight': [13],\n              'silent': [1],\n              'subsample': [0.7],\n              'colsample_bytree': [0.5],\n              'n_estimators': [100], #number of trees, change it to 1000 for better results\n              'missing':[-999],\n              'seed': [1337]}\n\nxgb= RandomizedSearchCV(estimator=xgb_model, param_distributions=parameters, n_jobs=-1, cv=3, verbose=2, random_state=1,n_iter=7,scoring='roc_auc')\n\n\n#xgb = GridSearchCV(xgb_model, parameters,scoring='roc_auc',n_jobs=-1)\n\n\nxgb.fit(X_train, y_train)\n\nxgb.best_params_\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ci-dessous, les paramètres idéaux de la régression linéaire.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = grid_obj.best_params_\nprint(xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators= LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=5,\n                   multi_class='ovr', n_jobs=None, penalty='l2',\n                   random_state=None, solver='sag', tol=0.0001, verbose=2,\n                   warm_start=False)\n\n\n\n\n\nxgb_clas=XGBClassifier(estimator=estimators, nthread=4,booster='gbtree',learning_rate=0.01,max_depth=14,\n                      min_child_weight=11,silent=1,subsample=0.5,colsample_bytree=0.5,\n                      n_estimators=500,missing=-999,seed=1337)\n\n# Fit the best algorithm to the data. \n\nxgb_clas.fit(X_train, y_train)\nprint('Train score', xgb_clas.score(X_train, y_train))\nprint('Test score', xgb_clas.score(X_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y2_pred = xgb_clas.predict_proba(X_test)[:,1]\n\n\nfpr, tpr, _ = roc_curve(y_test,y_pred)\nplt.plot(fpr, tpr, marker='.', label='LogistiIIc')\n\n\nprint(\"Le score AUC est: {} \".format(roc_auc_score(y_test, y2_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # <p id=\"fin\"> 11. Génération des résultats </p>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"resultat = xgb_clas.predict_proba(df_test)[:, 1]\nprint(resultat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_res = pd.DataFrame({'booking_id':test.index, 'return90':resultat})\ndf_res.to_csv('resultat_hugo.csv', index=False)\nresu = pd.read_csv('resultat_hugo.csv', index_col='booking_id')\nresu","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}